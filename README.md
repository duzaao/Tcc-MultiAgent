# Multi-Agent Conversational System with Model Context Protocol (MCP)

A secure, modular multi-agent system built with the Model Context Protocol (MCP) to enable safe and auditable interactions between language models and external services. This project demonstrates how intelligent agents can be deployed in enterprise environments while maintaining strong security boundaries and operational control.

## ğŸ“‹ Overview

This thesis explores the development of a conversational AI system where dedicated agents collaborate to interpret user requests and execute actions through a standardized protocol. By leveraging MCP, all operations remain transparent, auditable, and constrained to explicitly defined toolsâ€”eliminating the security risks of unrestricted approaches like direct Text-to-SQL.

The system uses a **multi-agent orchestration layer** where:
- A **Planner Agent** routes requests to the appropriate handler
- A **Tool Executor (MCP Agent)** performs actions through secure, isolated tools
- An **FAQ Agent** handles general queries and company information

Both **local LLMs** (via Ollama) and **API-based models** (OpenAI, Groq, etc.) are supported, enabling evaluation across different computational and privacy constraints.

## ğŸ¯ Research Questions

**RQ1:** How can a modular and reliable multi-agent architecture be designed using MCP?
- Focus: agent organization, MCP's role in tool invocation, maintaining security boundaries

**RQ2:** What are the performance trade-offs between local and API-based LLMs in MCP-driven systems?
- Focus: latency, accuracy, token usage, operational characteristics

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        User / Frontend Interface            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Agent Orchestrator â”‚
        â”‚  (Multi-Agent Layer) â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                               â”‚
    â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
    â”‚ Plannerâ”‚                â”‚  FAQ/Memory  â”‚
    â”‚ Agent  â”‚                â”‚   Agent      â”‚
    â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â”‚ (routes to)
        â”‚
    â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   MCP Agent         â”‚
    â”‚  (Tool Executor)    â”‚
    â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
    â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   MCP Server                 â”‚
    â”‚  (Tool Definitions & Calls)  â”‚
    â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
    â”Œâ”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚           â”‚          â”‚              â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”   â”Œâ”€â”€â”€â–¼â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”
â”‚Auth  â”‚   â”‚Flightsâ”‚   â”‚Login â”‚    â”‚ Externalâ”‚
â”‚API   â”‚   â”‚API    â”‚   â”‚Tools â”‚    â”‚APIs     â”‚
â””â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Project Structure

```
Tcc-MultiAgent/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ .env.example                 # Environment template (no credentials)
â”œâ”€â”€ .env                         # Local configuration (not committed)
â”‚
â”œâ”€â”€ agent/                       # Multi-agent orchestrator
â”‚   â”œâ”€â”€ agent.py                 # Main entry point
â”‚   â”œâ”€â”€ company_faq.md           # FAQ content
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ start.sh
â”‚   â””â”€â”€ multi/
â”‚       â”œâ”€â”€ agent_main.py        # Orchestrator entry point
â”‚       â”œâ”€â”€ agent_mcp.py         # MCP tool executor
â”‚       â”œâ”€â”€ agent_plan.py        # Router/planner agent
â”‚       â””â”€â”€ llm.py               # LLM provider abstraction
â”‚
â”œâ”€â”€ api/                         # Backend services
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ auth/                # Authentication service
â”‚   â”‚   â”œâ”€â”€ flights/             # Flight management service
â”‚   â”‚   â””â”€â”€ shared/              # Utilities
â”‚   â”œâ”€â”€ scripts/run_services.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ start.sh
â”‚
â”œâ”€â”€ mcp/                         # MCP server (tool definitions)
â”‚   â”œâ”€â”€ server_new.py            # MCP server implementation
â”‚   â”œâ”€â”€ mcp.json                 # Tool configuration
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ front/                       # Web frontend (optional)
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ app.js
â”‚   â””â”€â”€ style.css
â”‚
â”œâ”€â”€ infra/                       # Deployment & orchestration
â”‚   â”œâ”€â”€ docker-compose.yml       # Multi-container setup
â”‚   â”œâ”€â”€ startup.sh               # Service orchestration
â”‚   â””â”€â”€ test_stack.py            # Integration tests
â”‚
â”œâ”€â”€ terraform/                   # Infrastructure as Code (AWS/GCP/Azure)
â”‚   â”œâ”€â”€ main.tf
â”‚   â”œâ”€â”€ provider.tf
â”‚   â”œâ”€â”€ variables.tf
â”‚   â”œâ”€â”€ outputs.tf
â”‚   â””â”€â”€ terraform.tfvars
â”‚
â”œâ”€â”€ scripts/                     # Utility scripts
â”‚   â”œâ”€â”€ analyze.py               # Evaluation analyzer (Groq LLM)
â”‚   â””â”€â”€ analyze2.py              # Category-based analysis
â”‚
â”œâ”€â”€ data/                        # Test datasets
â”‚   â””â”€â”€ questions.jsonl          # Evaluation prompts
â”‚
â””â”€â”€ docs/                        # Documentation
    â”œâ”€â”€ ARCHITECTURE.md          # Detailed system design
    â”œâ”€â”€ SETUP.md                 # Installation & configuration
    â””â”€â”€ API.md                   # API reference
```

## ğŸš€ Quick Start

### Prerequisites

- Docker & Docker Compose (for containerized setup)
- Python 3.11+
- LLM Provider credentials (OpenAI, Groq, etc.) OR local Ollama

### Setup with Docker Compose (Recommended)

1. **Clone and configure:**
   ```bash
   git clone <repo>
   cd Tcc-MultiAgent
   cp .env.example .env
   # Edit .env with your LLM credentials and settings
   ```

2. **Start all services:**
   ```bash
   cd infra
   docker-compose -f docker-compose.yml up -d --build
   ```

   This starts:
   - MongoDB (data persistence)
   - API services (Auth & Flights)
   - Agent (multi-agent orchestrator with MCP)

   **Alternative:** Use `./startup.sh` for sequential startup with health checks.

3. **Access the system:**
   - Agent API: `http://localhost:8000`
   - Auth service: `http://localhost:8001`
   - Flights service: `http://localhost:8002`
   - MCP server: `http://localhost:8003` (internal, spawned by agent)

### Cloud Deployment

For AWS, GCP, or Azure deployment, use the Terraform configuration:

```bash
cd terraform
terraform init
terraform plan
terraform apply
```

See `terraform/` folder for infrastructure as code setup.


## ğŸ”§ Configuration

### Environment Variables (.env)

**LLM Configuration:**
```bash
# Choose provider: openai, groq, ollama, anthropic, etc.
LLM_PROVIDER=openai
MODEL=gpt-4o-mini

# API Keys (if using external providers)
OPENAI_API_KEY=sk-...
GROQ_API_KEY=gsk_...

# For local LLM via Ollama
OLLAMA_HOST=http://localhost:11434
# MODEL=llama3.2:3b (for local models)
```

**Database:**
```bash
MONGODB_URI=mongodb://localhost:27017/authsvc
MONGODB_DB=authsvc
```

**Security:**
```bash
JWT_SECRET=your-secret-key-change-this
JWT_ISSUER=authsvc
JWT_AUDIENCE=api
```

**Pricing Estimation (Optional):**
```bash
LLM_INPUT_COST_PER_1K=0.00150   # $ per 1K tokens
LLM_OUTPUT_COST_PER_1K=0.00600  # $ per 1K tokens
```

### Using Local LLM (Ollama)

To use local models instead of API calls:

1. **Install Ollama:** https://ollama.ai

2. **Pull a model:**
   ```bash
   ollama pull llama3.2:3b
   ```

3. **Enable in docker-compose.yml:**
   - Uncomment the `ollama` service in `infra/docker-compose.yml`
   - Update `.env`: `LLM_PROVIDER=ollama` and `MODEL=llama3.2:3b`

4. **Restart services:**
   ```bash
   docker-compose down
   ./startup.sh
   ```




## ğŸ” Security & MCP

### Key Security Features

1. **Isolated Tool Execution**: All operations through explicitly defined MCP tools
2. **No Direct Database Access**: SQL/database access forbidden; only through APIs
3. **Audit Trail**: Every tool invocation is logged and traceable
4. **Schema Validation**: Tool parameters validated before execution
5. **Authentication**: Login required for sensitive operations
6. **Session Management**: Stateful connections with timeout handling

### MCP Tools Available

- `login` - User authentication
- `get_flights` - Query available flights
- `get_tickets` - Retrieve user's tickets
- `buy_ticket` - Purchase a flight
- `cancel_ticket` - Cancel an existing ticket
- `get_flight_details` - Flight information

See `mcp/mcp.json` for complete tool definitions.

## ğŸ“ˆ Supported LLM Providers

| Provider | Model Examples | Notes |
|----------|---|---|
| **OpenAI** | gpt-4o-mini, gpt-4-turbo | Fastest, most capable, paid |
| **Groq** | llama-3.1-8b, qwen-32b | Fast inference, free tier available |
| **Anthropic** | claude-3-haiku | Good reasoning, paid |
| **Local (Ollama)** | llama3.2:3b, llama3:8b | Free, private, resource-intensive |

## ğŸ“ JSONL Format

Test questions file format:

```jsonl
{"id": 1, "prompt": "What flights are available from SÃ£o Paulo?", "category": "query"}
{"id": 2, "prompt": "Book flight ZZ999", "category": "action"}
{"id": 3, "prompt": "What's your refund policy?", "category": "faq"}
```

Expected fields:
- `id` - Unique identifier
- `prompt` - User question/request
- `category` - Classification (query, action, faq, login_auth, etc.)

## ğŸ—‚ï¸ Important Files

| File | Purpose |
|------|---------|
| `agent/multi/agent_main.py` | Main orchestrator entry point |
| `agent/multi/agent_plan.py` | Planner agent (routing logic) |
| `agent/multi/agent_mcp.py` | MCP agent (tool executor) |
| `agent/multi/llm.py` | LLM provider wrapper |
| `mcp/server_new.py` | MCP server implementation |
| `mcp/mcp.json` | Tool definitions & configuration |
| `api/src/auth/service.py` | Authentication logic |
| `api/src/flights/service.py` | Flight management logic |
| `infra/docker-compose.yml` | Multi-container orchestration |



## ğŸ“Š Performance Characteristics

### Latency (approximate, varies by model/network)

| Component | Time |
|-----------|------|
| Planner routing | 100-500ms |
| Tool execution | 500-2000ms |
| API calls | 200-800ms |
| **Total end-to-end** | **1000-4000ms** |

### Token Usage

Typically 1500-2500 tokens per interaction (prompt + response)

### Cost Example

Using GPT-4o-mini at $0.0015/1K input, $0.006/1K output:
- ~$0.004-0.010 per user interaction
- 1000 interactions = ~$4-10

## ğŸ¤ Contributing

This is a research/thesis project. For contributions:
1. Ensure code follows project style
2. Add tests for new features
3. Update documentation
4. Test with multiple LLM providers



## ğŸ” Key Takeaways

1. **MCP enables secure multi-agent systems** by constraining operations to explicitly defined tools
2. **Local LLMs offer privacy** but trade performance for cost
3. **Multi-agent orchestration improves modularity** through specialized components
4. **Enterprise AI requires strong boundaries** â€” unrestricted tool access is a security risk

---

**Last Updated:** December 2025  
**Status:** Active Development  
**Python Version:** 3.11+  
**Main Dependencies:** LangChain, FastAPI, MongoDB, MCP Protocol